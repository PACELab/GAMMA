.neo4j.conf
----
kafka.zookeeper.connect=localhost:2181
kafka.bootstrap.servers=localhost:9092
kafka.acks=1
kafka.num.partitions=1
kafka.retries=2
kafka.batch.size=16384
kafka.buffer.memory=33554432
kafka.reindex.batch.size=1000
kafka.session.timeout.ms=15000
kafka.connection.timeout.ms=10000
kafka.replication=1
kafka.linger.ms=1
kafka.transactional.id=
kafka.topic.discovery.polling.interval=300000

streams.source.topic.nodes.<TOPIC_NAME>=<PATTERN>
streams.source.topic.relationships.<TOPIC_NAME>=<PATTERN>
streams.source.enabled=<true/false, default=true>
streams.source.schema.polling.interval=<MILLIS, the polling interval for getting the schema information>
----

[NOTE]
====
**To use the Kafka transactions please set `kafka.transactional.id` and `kafka.acks` properly**.
Checkout this https://www.confluent.io/blog/transactions-apache-kafka/[blog post] for further details
about transactions in Apache Kafka
====

See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

In case you Kafka broker is configured with `auto.create.topics.enable` to `false`,
all the messages sent to topics that don't exist are discarded;
this because the `KafkaProducer.send()` method blocks the execution, as explained in https://issues.apache.org/jira/browse/KAFKA-3539[KAFKA-3539].
You can tune the custom property `kafka.topic.discovery.polling.interval` in order to
periodically check for new topics into the Kafka cluster so the plugin will be able
to send events to the defined topics.